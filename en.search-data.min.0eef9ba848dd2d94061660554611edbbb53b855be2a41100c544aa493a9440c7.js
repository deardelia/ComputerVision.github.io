'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/docs/data_preparation/','title':"Data Preparation",'content':"Instructions to prepare your own data for Boxer Data Set Composition The data set used for Boxer consists of 3 files:\n  results.csv: This file displays the prediction results of each classifier on each instance, where each row represents \u0026ldquo;id\u0026rdquo; of the data sample and each column represents the name of classifier.   features.csv: This file displays the features of each instance, where each row represents \u0026ldquo;id\u0026rdquo; of the data sample and each column represents the name of features.   manifest.json: This file is the configuration of the data set, which helps Boxer system to read and load the data set.\n  { \u0026quot;datasetName\u0026quot;: \u0026quot;IMDB 5000 dataset\u0026quot;, // name of the data set \u0026quot;classes\u0026quot;: [ // array including the names of classes \u0026quot;HIGH\u0026quot;, \u0026quot;MED\u0026quot;, \u0026quot;LOW\u0026quot; ], \u0026quot;classifiers\u0026quot;: [ // array including the names of classifiers \u0026quot;LR\u0026quot;, \u0026quot;KNN\u0026quot;, \u0026quot;LDA\u0026quot;, \u0026quot;NB\u0026quot;, \u0026quot;SVM\u0026quot;, \u0026quot;SVM-W\u0026quot;, \u0026quot;RF\u0026quot;, \u0026quot;RF-W\u0026quot;, \u0026quot;DT\u0026quot; ], \u0026quot;features\u0026quot;: { // dict including the detailed information of each feature: \u0026quot;type\u0026quot;, \u0026quot;description\u0026quot;, \u0026quot;categories\u0026quot;(optional), \u0026quot;bounds\u0026quot;(optional) \u0026quot;train_or_test\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;categorical\u0026quot;, \u0026quot;categories\u0026quot;: [ \u0026quot;train\u0026quot;, \u0026quot;test\u0026quot; ], \u0026quot;description\u0026quot;: \u0026quot;whether instance was used for training or testing\u0026quot; }, \u0026quot;movie_title\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;nominal\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;title\u0026quot; }, \u0026quot;title_year\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;interval\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;year released\u0026quot; }, \u0026quot;content_rating\u0026quot;: { \u0026quot;type\u0026quot;: \u0026quot;ratio\u0026quot;, \u0026quot;description\u0026quot;: \u0026quot;content rating\u0026quot;, \u0026quot;bounds\u0026quot;: [ 0.0, 1.0 ] }, } } Load Data to Boxer "});index.add({'id':1,'href':'/docs/demo/','title':"Online Demo",'content':"click here\n  "});index.add({'id':2,'href':'/categories/','title':"Categories",'content':""});index.add({'id':3,'href':'/docs/','title':"Docs",'content':""});index.add({'id':4,'href':'/','title':"Introduction",'content':"Boxer: Interactive Comparison of Classification Results Motivation Machine learning practitioners often perform experiments that compare classification results. Users gather the results of different classifiers and/or data perturbations on a collection of testing examples. Results data are stored and analyzed for tasks such as model selection, hyper-parameter tuning, data quality assessment, fairnes testing, and gaining insight about the underlying data. Classifier comparison experiments are typically evaluated by summary statistics of model performance, such as accuracy, F1, and related metrics. These aggregate measures provide for a quick summary, but not detailed examination. Examining performance on different subsets of data can provide insights into the models (e.g., to understand performance for future improvement), the data (e.g., to understand data quality issues to improve cleaning), or the underlying phenomena (e.g., to identify potential causal relationships). Making decisions solely on aggregated data can lead to missing important aspects of classifier performance. To perform such closer examination, practitioners rely on scripting and existing tools in their standard workflows. The lack of specific tooling makes the process laborious and comparisons challenging, limiting how often experiments are examined in detail.\nMain Contribution We contribute a comprehensive approach for interactive comparison of machine learning classifier results. Our approach has been implemented in a prototype system called Boxer. We show how Boxer enables users to perform a variety of tasks in assessing machine learning systems.\nInnovations   The approach to classifier comparison that combines subset identification, metric selection, and comparative visualization to enable detailed comparison in classifier results.\n  The architecture of multiple selections and set algebra that allows users to flexibly link views and specify data subsets of interest.\n  Interactive techniques and visual designs that make the approach practical. These key ideas should be applicable in other systems for interactive comparison within complex data.\n  Example of Boxer System This figure shows how Boxerâ€™s flexible mechanisms can be used to predict whether a person will commit a crime within two years based on the data set contains 6,172 instances. Parallel Metrics view (A) shows the C3 classifier has better performance by all metrics. A histogram of race (F) selects Caucasian (cyan) and African-American (pink) instances. The Overall Performance view (B) shows C3's overall higher precision, but a lack of overlap with cyan. The Confusion Matrix (C) Grid view shows many false positives for African-Americans and many false negatives for Caucasians for C3. Histograms show the distribution of selected sets across the actual (D) and the C3-predicted class (E). The Performance Selection views in the third line compare accuracy (G) , precision (H) , and recall (I) for C3 on the subsets.\nDemo Video "});index.add({'id':5,'href':'/tags/','title':"Tags",'content':""});})();